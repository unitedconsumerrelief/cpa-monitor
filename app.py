import os
import json
import sqlite3
import asyncio
import logging
from datetime import datetime, timezone
from typing import Dict, List, Optional, Set, Any
from urllib.parse import urlparse
import re

from fastapi import FastAPI, HTTPException, Request, Header, Query, Depends
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pydantic import BaseModel
import google.auth
from google.auth.transport.requests import Request as GoogleRequest
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Ringba Webhook", version="1.0.0")

# Global variables
sheets_service = None
realtime_dids: Set[str] = set()
background_writer_queue = []
background_writer_lock = asyncio.Lock()
db_path = "/tmp/ringba.sqlite"

# Memory optimization: Limit queue size to prevent memory issues
MAX_QUEUE_SIZE = 100

# Environment variables
RINGBA_CAMPAIGNS = set(os.getenv("RINGBA_CAMPAIGNS", "").split(",")) if os.getenv("RINGBA_CAMPAIGNS") else set()
GOOGLE_CREDENTIALS_JSON = os.getenv("GOOGLE_CREDENTIALS_JSON")
MASTER_CPA_DATA = os.getenv("MASTER_CPA_DATA")

# Pydantic models
class RingbaWebhookData(BaseModel):
    call_id: str
    call_start_utc: str
    did_raw: str
    caller_id: Optional[str] = None
    duration_sec: Optional[int] = None
    disposition: Optional[str] = None
    campaign: Optional[str] = None
    target: Optional[str] = None
    publisher_id: Optional[str] = None
    publisher_name: Optional[str] = None
    payout: Optional[float] = None
    revenue: Optional[float] = None

class HealthResponse(BaseModel):
    ok: bool
    realtime_dids: int

# Utility functions
def normalize_did(did: str) -> str:
    """Extract last 10 digits from DID"""
    digits_only = re.sub(r'\D', '', did)
    return digits_only[-10:] if len(digits_only) >= 10 else digits_only

def extract_sheet_id(url_or_id: str) -> str:
    """Extract sheet ID from URL or return as-is if already an ID"""
    if url_or_id.startswith("http"):
        # Extract ID from Google Sheets URL
        match = re.search(r'/d/([a-zA-Z0-9-_]+)', url_or_id)
        return match.group(1) if match else url_or_id
    return url_or_id

def init_database():
    """Initialize SQLite database for deduplication"""
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS processed_calls (
            call_id TEXT PRIMARY KEY,
            processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    conn.commit()
    conn.close()

def is_duplicate(call_id: str) -> bool:
    """Check if call_id has already been processed"""
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute("SELECT 1 FROM processed_calls WHERE call_id = ?", (call_id,))
    result = cursor.fetchone()
    conn.close()
    return result is not None

def mark_processed(call_id: str):
    """Mark call_id as processed"""
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute("INSERT OR IGNORE INTO processed_calls (call_id) VALUES (?)", (call_id,))
    conn.commit()
    conn.close()

def init_google_sheets():
    """Initialize Google Sheets service"""
    global sheets_service
    
    if not GOOGLE_CREDENTIALS_JSON:
        raise ValueError("GOOGLE_CREDENTIALS_JSON environment variable is required")
    
    try:
        credentials_info = json.loads(GOOGLE_CREDENTIALS_JSON)
        credentials = service_account.Credentials.from_service_account_info(
            credentials_info,
            scopes=['https://www.googleapis.com/auth/spreadsheets']
        )
        sheets_service = build('sheets', 'v4', credentials=credentials)
        logger.info("Google Sheets service initialized successfully")
    except Exception as e:
        logger.error(f"Failed to initialize Google Sheets service: {e}")
        raise

def get_sheet_id():
    """Get the sheet ID from environment variable"""
    if not MASTER_CPA_DATA:
        raise ValueError("MASTER_CPA_DATA environment variable is required")
    return extract_sheet_id(MASTER_CPA_DATA)

async def ensure_headers_exist(sheet_name: str, headers: List[str]):
    """Ensure headers exist in the specified sheet"""
    try:
        sheet_id = get_sheet_id()
        range_name = f"{sheet_name}!A1"
        
        # Check if headers exist
        result = sheets_service.spreadsheets().values().get(
            spreadsheetId=sheet_id,
            range=range_name
        ).execute()
        
        values = result.get('values', [])
        if not values or values[0] != headers:
            # Headers don't exist or are different, add them
            body = {
                'values': [headers]
            }
            sheets_service.spreadsheets().values().update(
                spreadsheetId=sheet_id,
                range=range_name,
                valueInputOption='RAW',
                body=body
            ).execute()
            logger.info(f"Added headers to {sheet_name} sheet")
    except Exception as e:
        logger.error(f"Failed to ensure headers exist for {sheet_name}: {e}")

async def append_to_sheet(sheet_name: str, rows: List[List[Any]]):
    """Append rows to the specified sheet"""
    try:
        sheet_id = get_sheet_id()
        range_name = f"{sheet_name}!A:N"
        
        body = {
            'values': rows
        }
        
        sheets_service.spreadsheets().values().append(
            spreadsheetId=sheet_id,
            range=range_name,
            valueInputOption='RAW',
            insertDataOption='INSERT_ROWS',
            body=body
        ).execute()
        
        logger.info(f"Appended {len(rows)} rows to {sheet_name} sheet")
    except Exception as e:
        logger.error(f"Failed to append to {sheet_name} sheet: {e}")

async def read_sheet_data(sheet_name: str) -> List[List[Any]]:
    """Read all data from the specified sheet"""
    try:
        sheet_id = get_sheet_id()
        range_name = f"{sheet_name}!A:N"
        
        result = sheets_service.spreadsheets().values().get(
            spreadsheetId=sheet_id,
            range=range_name
        ).execute()
        
        return result.get('values', [])
    except Exception as e:
        logger.error(f"Failed to read from {sheet_name} sheet: {e}")
        return []

async def write_sheet_data(sheet_name: str, data: List[List[Any]]):
    """Write data to the specified sheet (overwrites existing data)"""
    try:
        sheet_id = get_sheet_id()
        range_name = f"{sheet_name}!A:N"
        
        body = {
            'values': data
        }
        
        sheets_service.spreadsheets().values().update(
            spreadsheetId=sheet_id,
            range=range_name,
            valueInputOption='RAW',
            body=body
        ).execute()
        
        logger.info(f"Wrote {len(data)} rows to {sheet_name} sheet")
    except Exception as e:
        logger.error(f"Failed to write to {sheet_name} sheet: {e}")

async def background_writer():
    """Background task to flush queued data to Google Sheets"""
    while True:
        try:
            await asyncio.sleep(2)  # Reduced from 5 to 2 seconds for faster processing
            
            async with background_writer_lock:
                # Check if there are any items to process (inside the lock)
                if not background_writer_queue:
                    continue
                
                # Ensure headers exist
                headers = ["call_id","call_start_utc","did_raw","did_canon","caller_id",
                         "duration_sec","disposition","campaign","target",
                         "publisher_id","publisher_name",
                         "payout","revenue","_ingested_at"]
                await ensure_headers_exist("Ringba Raw", headers)
                
                # Flush up to 25 rows (reduced from 50 to prevent memory issues)
                rows_to_flush = background_writer_queue[:25]
                background_writer_queue[:] = background_writer_queue[25:]
                
                if rows_to_flush:
                    logger.info(f"📊 WRITING TO SHEETS: {len(rows_to_flush)} rows")
                    # Log first few call IDs being written
                    call_ids = [row[0] for row in rows_to_flush[:5]]
                    logger.info(f"📋 Call IDs being written: {call_ids}")
                    
                    await append_to_sheet("Ringba Raw", rows_to_flush)
                    logger.info(f"✅ Successfully flushed {len(rows_to_flush)} rows to Google Sheets")
                        
        except Exception as e:
            logger.error(f"❌ Error in background writer: {e}")
            # Add small delay on error to prevent rapid retry loops
            await asyncio.sleep(5)

async def background_cache():
    """Background task to maintain realtime DIDs cache"""
    while True:
        try:
            await asyncio.sleep(300)  # Wait 5 minutes
            
            # Read Real Time tab
            data = await read_sheet_data("Real Time")
            if data and len(data) > 1:  # Skip header row
                global realtime_dids
                realtime_dids = set()
                for row in data[1:]:  # Skip header
                    if row and len(row) > 0:
                        # Assuming first column contains normalized DIDs
                        did = str(row[0]).strip()
                        if did:
                            realtime_dids.add(did)
                
                logger.info(f"Updated realtime DIDs cache: {len(realtime_dids)} DIDs")
                
        except Exception as e:
            logger.error(f"Error in background cache: {e}")

# Security dependencies (removed for simplicity)

# API Endpoints
@app.post("/ringba-webhook")
async def ringba_webhook(request: Request):
    """Handle incoming Ringba webhook data"""
    try:
        # Parse JSON body
        body = await request.json()
        
        # Log incoming webhook for debugging
        logger.info(f"📥 WEBHOOK RECEIVED: {json.dumps(body, indent=2)}")
        
        # Extract call data
        call_id = body.get("call_id")
        if not call_id:
            logger.error("❌ Missing call_id in webhook")
            raise HTTPException(status_code=400, detail="Missing call_id")
        
        # Check for duplicates
        if is_duplicate(call_id):
            logger.warning(f"🔄 Duplicate call_id ignored: {call_id}")
            return {"status": "duplicate"}
        
        # Normalize DID
        did_raw = body.get("did", "")
        did_canon = normalize_did(did_raw)
        
        # Get campaign info (no filtering - accept all calls from Ringba)
        campaign_name = body.get("campaignName", "")
        campaign_id = body.get("campaignId", "")
        caller_id = body.get("callerId", "")
        
        # Log call details
        logger.info(f"📞 PROCESSING CALL: ID={call_id}, Caller={caller_id}, Campaign={campaign_name or campaign_id}, Publisher={body.get('publisherName', 'Unknown')}")
        
        # Accept all calls - no campaign filtering
        logger.info("✅ No campaign filtering - accepting all calls")
        
        # Prepare row data
        row = [
            call_id,
            body.get("callStartUtc", ""),
            did_raw,
            did_canon,
            caller_id,
            body.get("durationSec", ""),
            body.get("disposition", ""),
            campaign_name or campaign_id,
            body.get("target", ""),
            body.get("publisherId", ""),
            body.get("publisherName", ""),
            body.get("payout", ""),
            body.get("revenue", ""),
            datetime.now(timezone.utc).isoformat()
        ]
        
        # Add to background writer queue with memory protection
        async with background_writer_lock:
            if len(background_writer_queue) >= MAX_QUEUE_SIZE:
                logger.warning(f"⚠️ Queue full ({len(background_writer_queue)}), forcing immediate write")
                # Force immediate write to prevent memory issues
                await append_to_sheet("Ringba Raw", [row])
                logger.info(f"📊 Emergency write: {call_id}")
            else:
                background_writer_queue.append(row)
                queue_size = len(background_writer_queue)
                logger.info(f"📝 Added to queue: {call_id} (Queue size: {queue_size})")
        
        # Mark as processed
        mark_processed(call_id)
        
        logger.info(f"✅ Successfully queued call for processing: {call_id}")
        return {"status": "queued"}
        
    except Exception as e:
        logger.error(f"❌ Error processing webhook: {e}")
        logger.error(f"Request body: {body if 'body' in locals() else 'Failed to parse'}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/admin/refresh-map")
async def refresh_map():
    """Build DID→Publisher map and update sheets"""
    try:
        # Read Ringba Raw data
        raw_data = await read_sheet_data("Ringba Raw")
        if not raw_data or len(raw_data) <= 1:
            raise HTTPException(status_code=400, detail="No data found in Ringba Raw sheet")
        
        headers = raw_data[0]
        rows = raw_data[1:]
        
        # Find column indices
        call_id_idx = headers.index("call_id") if "call_id" in headers else 0
        call_start_idx = headers.index("call_start_utc") if "call_start_utc" in headers else 1
        did_canon_idx = headers.index("did_canon") if "did_canon" in headers else 3
        publisher_name_idx = headers.index("publisher_name") if "publisher_name" in headers else 10
        publisher_id_idx = headers.index("publisher_id") if "publisher_id" in headers else 9
        campaign_idx = headers.index("campaign") if "campaign" in headers else 7
        
        # Build DID→Publisher map (latest call per DID)
        did_publisher_map = {}
        publisher_did_counts = {}
        
        for row in rows:
            if len(row) <= max(call_id_idx, call_start_idx, did_canon_idx, publisher_name_idx, publisher_id_idx, campaign_idx):
                continue

            did_canon = str(row[did_canon_idx]).strip()
            campaign = str(row[campaign_idx]).strip()
            
            # Only process DIDs in realtime cache and whitelisted campaigns
            if did_canon in realtime_dids and (not RINGBA_CAMPAIGNS or campaign in RINGBA_CAMPAIGNS):
                call_id = row[call_id_idx]
                call_start = row[call_start_idx]
                publisher_name = str(row[publisher_name_idx]).strip()
                publisher_id = str(row[publisher_id_idx]).strip()
                
                # Keep latest call per DID
                if did_canon not in did_publisher_map or call_start > did_publisher_map[did_canon]["last_seen_call_start"]:
                    did_publisher_map[did_canon] = {
                        "publisher_name": publisher_name,
                        "publisher_id": publisher_id,
                        "last_seen_call_start": call_start
                    }
        
        # Prepare DID Publisher Map data
        did_map_headers = ["did_canon", "publisher_name", "publisher_id", "last_seen_call_start"]
        did_map_data = [did_map_headers]
        
        for did_canon, info in did_publisher_map.items():
            did_map_data.append([
                did_canon,
                info["publisher_name"],
                info["publisher_id"],
                info["last_seen_call_start"]
            ])
            
            # Count DIDs per publisher
            publisher = info["publisher_name"]
            publisher_did_counts[publisher] = publisher_did_counts.get(publisher, 0) + 1
        
        # Prepare Publisher DID Counts data
        counts_headers = ["publisher", "did_count", "last_refreshed"]
        counts_data = [counts_headers]
        
        for publisher, count in publisher_did_counts.items():
            counts_data.append([
                publisher,
                count,
                datetime.now(timezone.utc).isoformat()
            ])
        
        # Write to sheets
        await write_sheet_data("DID Publisher Map", did_map_data)
        await write_sheet_data("Publisher DID Counts", counts_data)
        
        logger.info(f"Refreshed DID map: {len(did_publisher_map)} DIDs, {len(publisher_did_counts)} publishers")
        return {
            "status": "success",
            "did_count": len(did_publisher_map),
            "publisher_count": len(publisher_did_counts)
        }
        
    except Exception as e:
        logger.error(f"Error refreshing map: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/healthz", response_model=HealthResponse)
async def health_check():
    """Health check endpoint"""
    return HealthResponse(ok=True, realtime_dids=len(realtime_dids))

@app.get("/debug/stats")
async def debug_stats():
    """Debug endpoint to show current statistics"""
    try:
        import psutil
        
        # Count processed calls from database
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        cursor.execute("SELECT COUNT(*) FROM processed_calls")
        processed_count = cursor.fetchone()[0]
        conn.close()
        
        # Get queue size
        async with background_writer_lock:
            queue_size = len(background_writer_queue)
        
        # Get memory usage
        memory_info = psutil.virtual_memory()
        memory_usage_mb = memory_info.used / (1024 * 1024)
        memory_percent = memory_info.percent
        
        return {
            "status": "ok",
            "processed_calls": processed_count,
            "queue_size": queue_size,
            "realtime_dids": len(realtime_dids),
            "campaign_filtering": list(RINGBA_CAMPAIGNS) if RINGBA_CAMPAIGNS else "None",
            "google_sheets_configured": bool(GOOGLE_CREDENTIALS_JSON and MASTER_CPA_DATA),
            "memory_usage_mb": round(memory_usage_mb, 2),
            "memory_percent": memory_percent,
            "memory_warning": memory_usage_mb > 400  # Warning if over 400MB
        }
    except Exception as e:
        return {"status": "error", "error": str(e)}

# Startup and shutdown events
@app.on_event("startup")
async def startup_event():
    """Initialize services on startup"""
    try:
        # Initialize database
        init_database()
        
        # Initialize Google Sheets
        init_google_sheets()
        
        # Start background tasks
        asyncio.create_task(background_writer())
        asyncio.create_task(background_cache())
        
        logger.info("Application started successfully")
    except Exception as e:
        logger.error(f"Failed to start application: {e}")
        raise

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
